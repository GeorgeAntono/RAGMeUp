### Question 8: Word representations across different languages using SkipGram

| Field                  | With HYDE                                   | Without HYDE                                |
|------------------------|---------------------------------------------|---------------------------------------------|
| **Documents Found**    | document nr: 0 <br> source: data/Week 6 reading a - BERTopic-NeuralTopicModelingWithAClassBasedTF-IDFprocedure (1).pdf <br> provenance: 0.997 <br> 2020). CTM, for example, demonstrates the ad-<br>vantage of relying on pre-trained language models,<br>namely that future improvements in language mod-<br>els may translate into better topic models (Bianchi<br>et al., 2020a).<br>Several approaches have started simplifying the<br>topic building process by clustering word- and<br>document embeddings (Sia et al., 2020; Angelov,<br>2020). This clustering approach allows for a ﬂex-<br>ible topic model as the generation of the clusters<br>can be separated from the process of generating the <br> <br>document nr: 1 <br> source: data/Week 6 reading a - BERTopic-NeuralTopicModelingWithAClassBasedTF-IDFprocedure (1).pdf <br> provenance: 0.991 <br> convert sentences and paragraphs to dense vector<br>representations using pre-trained language models.<br>It achieves state-of-the-art performance on various<br>sentence embedding tasks (Reimers and Gurevych,<br>2020; Thakur et al., 2020).<br>These embeddings, however, are primarily used<br>to cluster semantically similar documents and not<br>directly used in generating the topics. Any other<br>embedding technique can be used for this purpose<br>if the language model generating the document em- <br> <br>document nr: 2 <br> source: data/Week 8 reading.pdf <br> provenance: 0.959 <br> sample the size cbetween 1 and 5. In order to sub-<br>sample the most frequent words, we use a rejection<br>threshold of 10−4 (for more details, see (Mikolov et<br>al., 2013b)). When building the word dictionary, we<br>keep the words that appear at least 5 times in the<br>training set. The step size γ0 is set to 0.025 for the<br>skipgram baseline and to 0.05 for both our model<br>and the cbow baseline. These are the default values<br>in the word2vec package and work well for our<br>model too. <br> <br>document nr: 3 <br> source: data/Week 3 reading c - Mykolo etal (2013)-Word2Vec.pdf <br> provenance: 0 <br> 6 Conclusion<br>In this paper we studied the quality of vector representations of words derived by various models on<br>a collection of syntactic and semantic language tasks. We observed that it is possible to train high<br>quality word vectors using very simple model architectures, compared to the popular neural network<br>models (both feedforward and recurrent). Because of the much lower computational complexity, it<br>is possible to compute very accurate high dimensional word vectors from a much larger data set. <br> <br>                            | orsemantics2024.pdf <br> provenance: 0 <br> t<br>neg<br> h [ s ( c<br>t<br>neg<br>· w )] w (6.39)<br                         |
| **LLM's Final Answer** | According to Document 2, the Skip-Gram model is mentioned as one of the baselines used in the study, where the step size γ0 is set to 0.025 for the skipgram baseline.<br><br>Source: Document 2, id: 99162db67c15bc1fdc65739f7877514b                          | To address the question regarding word representations across different languages using SkipGram, I'll rely on Document 1 and Document 10.<br><br>According to Document 1, titled "Enriching Word Vectors with Subword Information," the authors propose a method to improve vector representations for morphologically rich languages by incorporating character-level information. They suggest representing words as the sum of their constituent character n-grams, which helps capture the morphology of words.<br><br>Similarly, Document 10 discusses a new approach based on the SkipGram model, where each word is represented as a bag of character n-grams. This method enables computing word representations for words that didn't appear in the training data and allows for faster training on large corpora.<br><br>Both documents highlight the importance of considering subword information when learning word representations, especially for languages with complex morphology. By doing so, the models can better capture the relationships between words and their components, leading to improved performance in various NLP tasks.<br><br>Source: Documents 1 and 10, specifically "Enriching Word Vectors with Subword Information" and "A New Approach Based on the Skipgram Model."                       |
| **Time Taken**         | 36.7 seconds                            | 17.87 seconds                         |

