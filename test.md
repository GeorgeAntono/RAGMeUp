### Question 1: What are the main considerations or concerns of the LDA paper, and who does the paper reference for explaining the main concept of LDA?

| Field                  | With HYDE                                   | Without HYDE                                |
|------------------------|---------------------------------------------|---------------------------------------------|
| **Documents Found**    | document nr: 0 <br> source: data/Week 6 reading b -2021-SSCI-AComparativeStudyOfFuzzyTopicModelsAndLDAInTermsOfInterpretability.pdf <br> provenance: 0.993 <br> [3] Blei, D. M., Ng, A. Y ., & Jordan, M. I. (2003). Latent dirichlet<br>allocation. the Journal of machine Learning research, 3, 993-1022.<br>[4] Blei, D. M., & Lafferty, J. D. (2006). Dynamic topic models. In<br>Proceedings of the 23rd international conference on Machine learning<br>(pp. 113-120).<br>[5] Blei, D., & Lafferty, J. (2006). Correlated topic models. Advances in<br>neural information processing systems, 18, 147.<br>[6] Blei, D. M. (2012). Probabilistic topic models. Communications of the<br>ACM, 55(4), 77-84. <br> <br>document nr: 1 <br> source: data/week 6.pdf <br> provenance: 0.988 <br> structure of the documents and extract topics (e.g. BERTopic).<br>1) Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. Journal of machine Learning research, 3(Jan), 993-1022.<br>2) Grootendorst, M. (2022). BERTopic: Neural topic modeling with a class-based TF-IDF procedure. arXiv preprint arXiv:2203.05794.<br>3) Landauer, T. K., Foltz, P. W., & Laham, D. (1998). An introduction to latent semantic analysis. Discourse processes, 25(2-3), 259-284. <br> <br>document nr: 2 <br> source: data/Week 6 reading d - LatenDirichletAllocation (1).pdf <br> provenance: 0.981 <br> text modeling, the topic probabilities provide an explicit representation of a document. We present<br>efﬁcient approximate inference techniques based on variational methods and an EM algorithm for<br>empirical Bayes parameter estimation. We report results in document modeling, text classiﬁcation,<br>and collaborative ﬁltering, comparing to a mixture of unigrams model and the probabilistic LSI<br>model.<br>1. Introduction<br>In this paper we consider the problem of modeling text corpora and other collections of discrete <br> <br>document nr: 3 <br> source: data/week 6.pdf <br> provenance: 0 <br> JM2050 Natural Language Proces                            | document nr: 0 <br> source: data/Week 3 reading c - Mykolo etal (2013)-Word2Vec.pdf <br> provenance: 0.97 <br> the most successful concept is to use distributed representations of words [10]. For example, neural<br>network based language models signiﬁcantly outperform N-gram models [1, 27, 17].<br>1.1 Goals of the Paper<br>The main goal of this paper is to introduce techniques that can be used for learning high-quality word<br>vectors from huge data sets with billions of words, and with millions of words in the vocabulary. As<br>far as we know, none of the previously proposed architectures has been successfully trained on more<br>1 <br> <br>document nr: 1 <br> source: data/Week 6 reading d - LatenDirichletAllocation (1).pdf <br> provenance: 0.957 <br> conclusions.<br>2. Notation and terminology<br>We use the language of text collections throughout the paper, referring to entities such as “words,”<br>“documents,” and “corpora.” This is useful in that it helps to guide intuition, particularly when<br>we introduce latent variables which aim to capture abstract notions such as topics. It is important<br>to note, however, that the LDA model is not necessarily tied to text, and has applications to other <br> <br>document nr: 2 <br> source: data/Week 7 reading .pdf <br> provenance: 0.89 <br> supervised labels speciﬁc to the desired task. The main advantage of ﬁne-tuning is strong<br>performance on many benchmarks. The main disadvantages are the need for a new large<br>dataset for every task, the potential for poor generalization out-of-distribution [MPL19], and<br>the potential to exploit spurious features of the training data [GSL+18, NK19]. We focus<br>on task-agnostic performance, leaving ﬁne-tuning for future work. <br> <br>document nr: 3 <br> source: data/Week 9 deep_learning_1.pdf <br> provenance: 0 <br> GloVeHey! This is something that we do in LSA/LDA too! <br> <br>document nr: 4 <br> source: data/Week 2 reading - 2_TextProc_2023.pdf <br> provenance: 0 <br> How is                          |
| **LLM's Final Answer** | According to Document 0, the main considerations or concerns of the LDA paper are not explicitly mentioned. However, it is clear that the authors aimed to introduce a new method for topic modeling.<br><br>To explain the main concept of LDA, the authors reference the concept of "Latent Dirichlet Allocation" itself, which implies that they are building upon existing knowledge of Dirichlet distributions and allocation. <br><br>However, if we look at Document 1, which references the original LDA paper by Blei, Ng, and Jordan (2003), it seems that the authors of the LDA paper do not reference any specific work for explaining the main concept of LDA. Instead, they seem to be introducing a novel approach to topic modeling.<br><br>But if we take into account the context provided by Document 3, where the authors mention that they are using LDA and another technique called FLSA-W, it suggests that the main concern of the LDA paper might be its ability to create topics that can be interpreted by a domain expert.<br><br>It is also worth noting that the authors of the LDA paper do not mention any specific references for explaining the main concept of LDA, but rather focus on presenting their own contribution to the field of topic modeling.                          | According to Document 10, the main inferential problem that needs to be solved in order to use LDA is computing the posterior distribution of the hidden variables given a document: p(θ, z \| w, α, β) = p(θ, z, w \| α, β) / p(w \| α, β).<br><br>However, the main considerations or concerns of the LDA paper are not explicitly stated in the provided documents.<br><br>But according to Document 11, LDA assumes that documents are formed by a generative process, where each document is a distribution over C topics, and each topic is a distribution over M words. <br><br>As for the references, the papers referenced in the provided documents do not provide information about the authors of the original LDA paper. However, the authors mentioned in the provided documents are:<br><br>* Blei, David M., Ng, Andrew Y., and Jordan, Michael I. (2003) - although this is not directly mentioned in any of the provided documents but is widely known as the originators of the LDA algorithm.<br><br>The provided documents also mention other researchers' works like Hofmann (1999), GSL+18, NK19, MPL19, Rijcken et al. (2021), etc., but they are not the original authors of the LDA concept.                       |
| **Time Taken**         | 46.72 seconds                            | 23.69 seconds                         |

### Question 2: What are the main challenges of prompt engineering according to research?

| Field                  | With HYDE                                   | Without HYDE                                |
|------------------------|---------------------------------------------|---------------------------------------------|
| **Documents Found**    | document nr: 0 <br> source: data/Week 6 reading a - BERTopic-NeuralTopicModelingWithAClassBasedTF-IDFprocedure (1).pdf <br> provenance: 0.993 <br> 2020). CTM, for example, demonstrates the ad-<br>vantage of relying on pre-trained language models,<br>namely that future improvements in language mod-<br>els may translate into better topic models (Bianchi<br>et al., 2020a).<br>Several approaches have started simplifying the<br>topic building process by clustering word- and<br>document embeddings (Sia et al., 2020; Angelov,<br>2020). This clustering approach allows for a ﬂex-<br>ible topic model as the generation of the clusters<br>can be separated from the process of generating the <br> <br>document nr: 1 <br> source: data/Week 6 reading a - BERTopic-NeuralTopicModelingWithAClassBasedTF-IDFprocedure (1).pdf <br> provenance: 0.989 <br> score is the average of 15 separate runs.<br>model for BERTopic and CTM (Song et al., 2020).<br>Two variations of Top2Vec were modeled, one with<br>Doc2Vec and one with the "all-mpnet-base-v2"<br>SBERT model7.<br>For fair comparisons between BERTopic and<br>Top2Vec, the parameters of HDBSCAN and<br>UMAP were ﬁxed between topic models.<br>To measure the generalizability of BERTopic<br>across language models, four different language<br>models were used in the experiments with<br>BERTopic, namely the Universal Sentence Encoder <br> <br>document nr: 2 <br> source: data/Week 6 reading c 2023-IFSA-InterpretingTopicModelsWithChatGPT.pdf <br> provenance: 0.915 <br> trained using a language modeling objective, but it performs<br>no fine-tuning [28]. Instead, it uses textual prompts to perform<br>zero-shot inference on various tasks. Ever since the switch<br>from the pre-train and fine-tune paradigm to the pre-train,<br>prompt, and predict paradigm [22], there is a growing literature<br>on prompt engineering [29], [30]. After the GPT-2 proposal,<br>evidence suggested that LM performance strongly depends on<br>the scale and only weakly on the model s                            | document nr: 0 <br> source: data/Week 7.pdf <br> provenance: 0.942 <br> JM2050 Natural Language Processing 2024 – 2025<br>U. Kaymak 35<br>Advanced prompt engineering techniques<br>- Few shot prompts<br>- Chain of thought reasoning<br>- Effective graph of thought https://arxiv.org/abs/2305.16582<br>- Self-consistency https://openreview.net/forum?id=1PL1NIMMrw<br>- Knowledge generation prompting<br>-R e A C T<br>41 <br> <br>document nr: 1 <br> source: data/Week 7.pdf <br> provenance: 0.882 <br> JM2050 Natural Language Processing 2024 – 2025<br>U. Kaymak 19<br>Prompts and prompt engineering<br>19<br> Source: https://www.cse.iitd.ac.in/~mausam/courses/col772/spring2023/lectures/22-promptengg.pdf <br> <br>document nr: 2 <br> source: data/Week 7.pdf <br> provenance: 0.813 <br> JM2050 Natural Language Processing 2024 – 2025<br>U. Kaymak 31<br>Advanced Prompt <br>Engineering <br> <br>document nr: 3 <br> source: data/Week 5 guest a[Xudong Lu]LLM-medicine Research Overview.pdf <br> provenance: 0 <br> Discussions of Breast-COMA<br>➢ Privacy issue:<br>Using the ChatGPT API resulted in patients’ data being public, violating patients’ privacy rights.<br>➢ Security issue:<br>LLM lacks professionalism and interpretability, posing potential security risks to patients.<br>➢ Prompt engineering limitations:<br>Prompt design lacks clear optimization directions and relies on human experience.<br>Few-shot is extremely sensitive to example selection, distribution, and design.<br>It is difficult for LLM to balance accuracy and cost. <br> <br>document nr: 4 <br> source: data/Week 7 reading .pdf <br> provenance: 0.89 <br> supervised labels speciﬁc to the desired task. The main advantage of ﬁne-tuning is strong<br>performance on many benchmarks. The main disadvantages are the need for a new large<br>dataset for every task, the potential for poor generalization out-of-distribution [MPL19], and<br>the potential to exploit spurious features of the training data [GSL+18, NK19]. We focus<br>on task                         |
| **LLM's Final Answer** | According to Document 2, the main challenge of prompt engineering is that the performance of language models (LMs) strongly depends on the scale rather than the model shape, suggesting that finding the optimal scale is crucial for effective prompt engineering.<br><br>Source: Document 2, id: 007371725d9f1cf89f88b9e1221dc82d, source: data/Week 6 reading c 2023-IFSA-InterpretingTopicModelsWithChatGPT.pdf                          | According to Document 3, discussions of Breast-COMA, the main challenges of prompt engineering are:<br><br>1. **Privacy issue**: Using the ChatGPT API resulted in patients' data being public, violating patients' privacy rights.<br>2. **Security issue**: LLM lacks professionalism and interpretability, posing potential security risks to patients.<br>3. **Prompt engineering limitations**: Prompt design lacks clear optimization directions and relies on human experience. Few-shot is extremely sensitive to example selection, distribution, and design.<br><br>Additionally, Document 11 mentions that ground truth in prompts matters little, and instead, other key aspects of the demonstrations are key drivers in end-task performance, such as:<br><br>1. **Label space**<br>2. **Distribution of the input text**<br>3. **Overall format of the sequence**<br><br>These challenges highlight the complexities and nuances involved in designing effective prompts for language models like ChatGPT.                       |
| **Time Taken**         | 37.87 seconds                            | 16.26 seconds                         |

### Question 3: What are the different types of topic modeling 

| Field                  | With HYDE                                   | Without HYDE                                |
|------------------------|---------------------------------------------|---------------------------------------------|
| **Documents Found**    | document nr: 0 <br> source: data/week 6.pdf <br> provenance: 0.988 <br> structure of the documents and extract topics (e.g. BERTopic).<br>1) Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. Journal of machine Learning research, 3(Jan), 993-1022.<br>2) Grootendorst, M. (2022). BERTopic: Neural topic modeling with a class-based TF-IDF procedure. arXiv preprint arXiv:2203.05794.<br>3) Landauer, T. K., Foltz, P. W., & Laham, D. (1998). An introduction to latent semantic analysis. Discourse processes, 25(2-3), 259-284. <br> <br>document nr: 1 <br> source: data/Week 6 reading b -2021-SSCI-AComparativeStudyOfFuzzyTopicModelsAndLDAInTermsOfInterpretability.pdf <br> provenance: 0.983 <br> [3] Blei, D. M., Ng, A. Y ., & Jordan, M. I. (2003). Latent dirichlet<br>allocation. the Journal of machine Learning research, 3, 993-1022.<br>[4] Blei, D. M., & Lafferty, J. D. (2006). Dynamic topic models. In<br>Proceedings of the 23rd international conference on Machine learning<br>(pp. 113-120).<br>[5] Blei, D., & Lafferty, J. (2006). Correlated topic models. Advances in<br>neural information processing systems, 18, 147.<br>[6] Blei, D. M. (2012). Probabilistic topic models. Communications of the<br>ACM, 55(4), 77-84. <br> <br>document nr: 2 <br> source: data/Week 6 reading b -2021-SSCI-AComparativeStudyOfFuzzyTopicModelsAndLDAInTermsOfInterpretability.pdf <br> provenance: 0.87 <br> indicating the most important topics for its decisions.<br>A problem with current topic models is they do not always<br>give interpretable results. Fuzzy Latent Semantic Analysis<br>(FLSA) [19] is a topic modeling algorithm applied to medical<br>data and showed superior performance to the most popular<br>model Latent Dirichlet Allocation (LDA). FLSA is based<br>on grouping similar documents in terms of the words they<br>contain. The membership of each document to a topic is <br> <br>document nr: 3 <br> source: data/Week 6 reading c 2023-IFSA-InterpretingTopicModelsWithChatGPT.pdf                             | document nr: 0 <br> source: data/Week 6 reading b -2021-SSCI-AComparativeStudyOfFuzzyTopicModelsAndLDAInTermsOfInterpretability.pdf <br> provenance: 0.976 <br> The contributions of this paper are:<br>1) we propose two new topic modeling algorithms,<br>2) we compare four different topic models in terms of<br>interpretability,<br>3) we study the relation between various parameters of the<br>topic models and their properties that can could be useful<br>978-1-7281-9048-8/21/$31.00 ©2021 IEEE <br> <br>document nr: 1 <br> source: data/Week 6 reading b -2021-SSCI-AComparativeStudyOfFuzzyTopicModelsAndLDAInTermsOfInterpretability.pdf <br> provenance: 0.956 <br> determined by using fuzzy clustering. However, documents<br>might contain different number of topics and may contain<br>different subsets of words in a topic, which inﬂuences the<br>quality of the clustering results. This leads typically to large<br>overlap between topics, which reduces the interpretability of<br>the topic model obtained. In this paper, we propose two topic<br>modeling methods that can reduce the overlap between the<br>topics. The ﬁrst model, FLSA-W, clusters on words rather <br> <br>document nr: 2 <br> source: data/Week 9 deep_learning_1.pdf <br> provenance: 0.904 <br> Auto-encodersBut you can also do topic modeling with this <br> <br>document nr: 3 <br> source: data/Week 6 reading c 2023-IFSA-InterpretingTopicModelsWithChatGPT.pdf <br> provenance: 0 <br> the potential of using ChatGPT, the state-of-the-art in LLMs,<br>to interpret the output generated by topic models. We work<br>with topics trained on Electronic Health Records (EHRs)<br>and compare the interpretations from ChatGPT and a domain<br>expert.<br>We follow a three-step approach. In the first step, we create<br>topics generated by the popular probabilistic topic modeling<br>algorithm Latent Dirichlet Allocation (LDA) [12]. Because of<br>the nature of topic modeling, where words belong to different <br> <br>document nr: 4 <br> source: data                         |
| **LLM's Final Answer** | Based on the provided documents, there are several types of topic modeling mentioned.<br><br>According to Document 1, "Latent Dirichlet Allocation" (LDA) is a type of topic modeling algorithm developed by Blei et al. (2003) [1]. It is a probabilistic model that infers the underlying topic structure of a corpus based on the distribution of words.<br><br>Document 2 mentions "Fuzzy Latent Semantic Analysis" (FLSA) as another type of topic modeling algorithm, which is an improvement over traditional LDA. FLSA groups similar documents based on the words they contain and provides more interpretable results.<br><br>Additionally, Document 3 introduces "Fuzzy Topic Modeling" (FTM) algorithms, such as FLSA-W, which considers the degree of membership of a word to a topic. This approach allows for more nuanced understanding of the relationships between words and topics.<br><br>Furthermore, Document 1 also mentions "Correlated Topic Models" (CTMs) developed by Blei and Lafferty (2006) [5], which extend the basic LDA model by incorporating correlations between topics.<br><br>Lastly, Document 1 references "Dynamic Topic Models" (DTMs) proposed by Blei and Lafferty (2006) [4], which allow for time-evolving topics and are useful for analyzing temporal data.<br><br>These are some of the different types of topic modeling techniques mentioned in the provided documents.                          | According to Document 4, some of the popular topic modeling techniques include:<br><br>* Latent Dirichlet Allocation (LDA) [3]<br>* Topics over Time model [48]<br>* Dynamic Topic Model [4]<br>* Pachinko Allocation Model [22]<br>* Correlated Topic Model [5]<br><br>Additionally, Document 1 mentions two proposed topic modeling methods that can reduce overlap between topics: FLSA-W and another method.<br><br>Furthermore, Document 5 explains that traditional topic modeling algorithms return two matrices, but this need not be the case for all models.<br><br>Lastly, Document 8 describes the output of topic models as consisting of two matrices: P(Wi\|Tk) and P(Tk\|Dj).<br><br>These are some examples of the different types of topic modeling techniques mentioned in the provided documents.                       |
| **Time Taken**         | 55.43 seconds                            | 16.12 seconds                         |

### Question 4: What are the limitations of BERTopic?

| Field                  | With HYDE                                   | Without HYDE                                |
|------------------------|---------------------------------------------|---------------------------------------------|
| **Documents Found**    | document nr: 0 <br> source: data/Week 6 reading b -2021-SSCI-AComparativeStudyOfFuzzyTopicModelsAndLDAInTermsOfInterpretability.pdf <br> provenance: 0.997 <br> ACM, 55(4), 77-84.<br>[7] Devlin, J., Chang, M., Lee, K., & Toutanova, K. (2019). BERT: Pre-<br>training of Deep Bidirectional Transformers for Language Understand-<br>ing. Proceedings of the 2019 Conference of the North American Chapter<br>of the Association for Computational Linguistics: Human Language<br>Technologies, V olume 1 (4171–4186)<br>[8] Dieng, A. B., Ruiz, F. J., & Blei, D. M. (2020). Topic modeling in<br>embedding spaces. Transactions of the Association for Computational<br>Linguistics, 8, 439-453. <br> <br>document nr: 1 <br> source: data/Week 6 reading b -2021-SSCI-AComparativeStudyOfFuzzyTopicModelsAndLDAInTermsOfInterpretability.pdf <br> provenance: 0.992 <br> [48] Wang, X., & McCallum, A. (2006). Topics over time: a non-markov<br>continuous-time model of topical trends. In Proceedings of the 12th<br>ACM SIGKDD international conference on Knowledge discovery and<br>data mining (pp. 424-433).<br>[49] Zuo, Y ., Wu, J., Zhang, H., Lin, H., Wang, F., Xu, K., & Xiong, H.<br>(2016). Topic modeling of short texts: A pseudo-document view. In<br>Proceedings of the 22nd ACM SIGKDD international conference on<br>knowledge discovery and data mining (pp. 2105-2114). <br> <br>document nr: 2 <br> source: data/Week 6 reading a - BERTopic-NeuralTopicModelingWithAClassBasedTF-IDFprocedure (1).pdf <br> provenance: 0.979 <br> score is the average of 15 separate runs.<br>model for BERTopic and CTM (Song et al., 2020).<br>Two variations of Top2Vec were modeled, one with<br>Doc2Vec and one with the "all-mpnet-base-v2"<br>SBERT model7.<br>For fair comparisons between BERTopic and<br>Top2Vec, the parameters of HDBSCAN and<br>UMAP were ﬁxed between topic models.<br>To measure the generalizability of BERTopic<br>across language models, four different language<br>models were used in the experiments with<br>BERTopic, na                            | document nr: 0 <br> source: data/Week 7 reading .pdf <br> provenance: 0.98 <br> matching the performance of state-of-the-art ﬁne-tuned systems, as well as generating high-quality<br>samples and strong qualitative performance at tasks deﬁned on-the-ﬂy. We documented roughly<br>predictable trends of scaling in performance without using ﬁne-tuning. We also discussed the social<br>impacts of this class of model. Despite many limitations and weaknesses, these results suggest that<br>very large language models may be an important ingredient in the development of adaptable, general<br>language systems. <br> <br>document nr: 1 <br> source: data/Week 6 reading a - BERTopic-NeuralTopicModelingWithAClassBasedTF-IDFprocedure (1).pdf <br> provenance: 0.97 <br> perspective.<br>In this paper, we introduce BERTopic, a topic<br>arXiv:2203.05794v1  [cs.CL]  11 Mar 2022 <br> <br>document nr: 2 <br> source: data/Week 7 reading .pdf <br> provenance: 0.917 <br> Our goal is not to exhaustively characterize GPT-3, but to give a preliminary analysis of some of<br>its limitations and behaviors. We focus on biases relating to gender, race, and religion, although<br>many other categories of bias are likely present and could be studied in follow-up work. This is a<br>preliminary analysis and does not reﬂect all of the model’s biases even within the studied categories.<br>Broadly, our analysis indicates that internet-trained models have internet-scale biases; models tend to <br> <br>document nr: 3 <br> source: data/Week 7.pdf <br> provenance: 0 <br> JM2050 Natural Language Processing 2024 – 2025<br>U. Kaymak 67<br>Limitations and considerations of ToT<br>74<br>Disadvantage: Computational requirements: at worst, ToT has an exponential <br>number of calls with increasing lengths per call. <br>Considerations: Deliberate search/reasoning methods might not be necessary for <br>many existing tasks. <br>However, as LMs are being deployed for more real-world decision <br>making applications, more comple                         |
| **LLM's Final Answer** | According to Document 2, which discusses BERTopic-Neural Topic Modeling With A Class-Based TF-IDF procedure, there isn't an explicit discussion about the limitations of BERTopic. However, I can infer some potential limitations based on related research.<br><br>One possible limitation of BERTopic could be its reliance on pre-trained language models like SBERT or Doc2Vec, which might not perform well on low-resource languages or domains where these models haven't been extensively trained. This is because BERTopic leverages the strengths of these pre-trained models to generate topics.<br><br>Another potential limitation could be the computational cost associated with training BERTopic, especially when working with large datasets. As mentioned in Document 2, the authors used multiple language models and fixed parameters for comparison purposes, indicating that BERTopic may require significant computational resources.<br><br>Additionally, Document 1 discusses various topic modeling techniques, including those that don't rely on pre-trained language models. These alternatives might offer more flexibility and adaptability to specific domains or languages, potentially making them more suitable for certain applications.<br><br>Please note that these are inferred limitations and not explicitly stated in the provided documents. For a comprehensive understanding of BERTopic's limitations, it would be best to consult the original paper by Song et al. (2020) or other relevant sources.                          | According to Document 5, "BERTopic assumes that each document only contains a single topic" which is a limitation of the model. Additionally, Document 12 mentions that the topic representation itself does not directly account for contextual relationships between words, making the interpretation of topics potentially redundant.<br><br>Furthermore, Document 13 highlights that while BERTopic has several strengths, there are also weaknesses to consider. Specifically, it notes that the model assumes a single topic per document, which may not accurately capture complex or multi-topic documents.<br><br>Source: Documents 5, 12, and 13.                       |
| **Time Taken**         | 70.46 seconds                            | 13.46 seconds                         |

### Question 5: how do researchers propose attention models to work?What is the methodology of word2vec paper

| Field                  | With HYDE                                   | Without HYDE                                |
|------------------------|---------------------------------------------|---------------------------------------------|
| **Documents Found**    | document nr: 0 <br> source: data/Week 3 reading b - vectorsemantics2024.pdf <br> provenance: 0.991 <br> this context window is to the target word. The probability is based on applying the<br>logistic (sigmoid) function to the dot product of the embeddings of the target word<br>with each context word. To compute this probability, we just need embeddings for<br>each target word and context word in the vocabulary.<br>1<br>W<br>C<br>aardvark<br>zebra<br>zebra<br>aardvark<br>apricot<br>apricot<br>\|V\|<br>\|V\|+1<br>2V<br>& =<br>target words<br>context & noise<br>words<br>…<br>…<br>1..d<br>…<br>…<br>Figure 6.13 The embeddings learned by the skipgram model. The algorithm stores two <br> <br>document nr: 1 <br> source: data/Week 3 reading b - vectorsemantics2024.pdf <br> provenance: 0.989 <br> this context window is to the target word. The probability is based on applying the<br>logistic (sigmoid) function to the dot product of the embeddings of the target word<br>with each context word. We could thus compute this probability if only we had<br>embeddings for each target word and context word in the vocabulary. Let’s now turn<br>to learning these embeddings (which is the real goal of training this classiﬁer in the<br>ﬁrst place).<br>6.8.2 Learning skip-gram embeddings <br> <br>document nr: 2 <br> source: data/Week 3 reading b - vectorsemantics2024.pdf <br> provenance: 0.984 <br> Loss function for one w with cpos, cneg1...cnegkMaximize the similarity of the target with the actual context words, and minimize the similarity of the target with the k negative sampled non-neighbor words. <br>6.8 • W ORD 2 VEC 21<br>Given the set of positive and negative training instances, and an initial set of embed-<br>dings, the goal of the learning algorithm is to adjust those embeddings to<br>• Maximize the similarity of the target word, context word pairs ( w , c pos ) drawn<br>from the positive examples <br> <br>                            | document nr: 0 <br> source: data/Week 6 reading b -2021-SSCI-AComparativeStudyOfFuzzyTopicModelsAndLDAInTermsOfInterpretability.pdf <br> provenance: 0.959 <br> The contributions of this paper are:<br>1) we propose two new topic modeling algorithms,<br>2) we compare four different topic models in terms of<br>interpretability,<br>3) we study the relation between various parameters of the<br>topic models and their properties that can could be useful<br>978-1-7281-9048-8/21/$31.00 ©2021 IEEE <br> <br>document nr: 1 <br> source: data/Week 10deep_learning_2.pdf <br> provenance: 0.91 <br> Attention modelsThis is what attention does! <br> <br>document nr: 2 <br> source: data/Week 6 reading d - LatenDirichletAllocation (1).pdf <br> provenance: 0.712 <br> IR researchers for text corpora—a methodology successfully deployed in modern Internet search<br>engines—reduces each document in the corpus to a vector of real numbers, each of which repre-<br>sents ratios of counts. In the populartf-idf scheme (Salton and McGill, 1983), a basic vocabulary<br>of “words” or “terms” is chosen, and, for each document in the corpus, a count is formed of the<br>number of occurrences of each word. After suitable normalization, this term frequency count is <br> <br>document nr: 3 <br> source: data/Week 12 b deep_learning_3.pdf <br> provenance: 0 <br> Attention modelsWhat about text classification?Well maybe with some prompt enineeringSpeaking of which… <br> <br>document nr: 4 <br> source: data/Week 6 reading d - LatenDirichletAllocation (1).pdf <br> provenance: 0 <br> data. The goal is to ﬁnd short descriptions of the members of a collection that enable efﬁcient<br>processing of large collections while preserving the essential statistical relationships that are useful<br>for basic tasks such as classiﬁcation, novelty detection, summarization, and similarity and relevance<br>judgments.<br>Signiﬁcant progress has been made on this problem by researchers in the ﬁeld of informa-<br>tion retrieval (IR)                         |
| **LLM's Final Answer** | According to the provided documents, I found information related to the Word2Vec paper and its methodology.<br><br>In the Word2Vec paper, researchers propose a method called "skip-gram" to learn word embeddings. This involves computing the probability of a target word given its context words within a certain window size (as shown in Document 0 and Document 1).<br><br>To achieve this, they apply the logistic (sigmoid) function to the dot product of the embeddings of the target word with each context word. However, they don't directly provide the embeddings for each target word and context word; instead, they aim to learn these embeddings through the training process.<br><br>The learning algorithm (described in Document 2) maximizes the similarity of the target word with the actual context words and minimizes the similarity with the negative sampled non-neighbor words. This is done by adjusting the embeddings to optimize the loss function.<br><br>The key steps of the Word2Vec methodology involve:<br><br>1. Computing the probability of a target word given its context words using the logistic function and dot products of embeddings.<br>2. Learning the embeddings through maximizing the similarity of target-context word pairs and minimizing the similarity with negative samples.<br><br>Source: Documents 0, 1, and 2 from "data/Week 3 reading b - vectorsemantics2024.pdf".                          | To answer your question, I'll refer to Document 5, which discusses the comparison of attention models to word2vec.<br><br>According to Document 5, "Attention models" (source: data/Week 10 deep_learning_2.pdf), researchers propose attention models to work by focusing on specific parts of the input sequence that are most relevant to the task at hand. This is in contrast to traditional methods like word2vec, which treats all words equally and assigns them the same weight.<br><br>Regarding the methodology of the word2vec paper, it is not explicitly mentioned in the provided documents. However, based on general knowledge, the word2vec paper proposes a method for representing words as vectors in a high-dimensional space, where semantically similar words are mapped to nearby points. The methodology involves training a neural network to predict the context words given a target word, and the weights learned during this process are used to compute the vector representation of each word.<br><br>For more information on the word2vec paper, please consult the original research paper by Mikolov et al. (2013).<br><br>Source: Document 5, "Attention models", source: data/Week 10 deep_learning_2.pdf                       |
| **Time Taken**         | 46.5 seconds                            | 19.25 seconds                         |

### Question 6: What model is faster than LDA?

| Field                  | With HYDE                                   | Without HYDE                                |
|------------------------|---------------------------------------------|---------------------------------------------|
| **Documents Found**    | document nr: 0 <br> source: data/Week 3 reading a - nb24aug.pdf <br> provenance: 0.49 <br> Text Classification and Naive BayesSentiment and Binary Naive Bayes <br> <br>document nr: 1 <br> source: data/Week 3 reading a - nb24aug.pdf <br> provenance: 0.423 <br> Text Classification and Naive BayesThe Naive Bayes Classifier <br> <br>                            | document nr: 0 <br> source: data/Week 2 reading - 2_TextProc_2023.pdf <br> provenance: 0.966 <br> Capture groups: multiple registers/the (.*)er they (.*), the \1er we \2/ Matchesthe faster they ran, the faster we ranBut notthe faster they ran, the faster we ate <br> <br>document nr: 1 <br> source: data/Week 9 reading.pdf <br> provenance: 0.961 <br> in Table 2) outperforms the best previously reported models (including ensembles) by more than 2.0<br>BLEU, establishing a new state-of-the-art BLEU score of 28.4. The conﬁguration of this model is<br>listed in the bottom line of Table 3. Training took 3.5 days on 8 P100 GPUs. Even our base model<br>surpasses all previously published models and ensembles, at a fraction of the training cost of any of<br>the competitive models. <br> <br>document nr: 2 <br> source: data/Week 2.pdf <br> provenance: 0.904 <br> JM2050 Natural Language Processing 2024 –2025<br>Uzay Kaymak 50<br>www.jads.nl<br>Capture groups: multiple registers<br>the (.*)er they (.*), the \\1er we \\2 <br>Matches<br>the faster they ran, the faster we ran<br>But not<br>the faster they ran, the faster we ate <br> <br>document nr: 3 <br> source: data/Week 6 reading d - LatenDirichletAllocation (1).pdf <br> provenance: 0 <br> deed, as we discuss in Section 5, we adopt the empirical Bayes approach to estimating parameters<br>such asα and β in simple implementations of LDA, but we also consider fuller Bayesian approaches<br>as well.<br>997 <br> <br>document nr: 4 <br> source: data/Week 9 reading.pdf <br> provenance: 0 <br> different layer types.<br>As noted in Table 1, a self-attention layer connects all positions with a constant number of sequentially<br>executed operations, whereas a recurrent layer requires O(n) sequential operations. In terms of<br>computational complexity, self-attention layers are faster than recurrent layers when the sequence<br>length n is smaller than the representation dimensionality d, which is most often the case with <br> <br>document nr:                         |
| **LLM's Final Answer** | To determine which model is faster than Latent Dirichlet Allocation (LDA), I'll refer to Document 1: "Text Classification and Naive Bayes" (Source: data/Week 3 reading a - nb24aug.pdf).<br><br>According to this document, Naive Bayes is generally faster than LDA because it has fewer parameters to estimate and doesn't require complex computations like LDA does.<br><br>In fact, the document states that Naive Bayes is often preferred when speed is crucial in text classification tasks, especially for large datasets. <br><br>Therefore, based on this information, Naive Bayes is indeed faster than LDA.                          | According to Document 4, "self-attention layers are faster than recurrent layers when the sequence length n is smaller than the representation dimensionality d." However, I did not find information about a model being faster than LDA directly.<br><br>However, according to Document 1, the author mentions that their model outperforms the best previously reported models, including ensembles, and surpasses all previously published models and ensembles, at a fraction of the training cost of any of the competitive models. But it does not explicitly state that it is faster than LDA.<br><br>On the other hand, Document 6 states that "CTM and LDA, are faster than the neural network-based" model.                       |
| **Time Taken**         | 35.83 seconds                            | 14.71 seconds                         |

### Question 7: What is the fastest topic modeling system?

| Field                  | With HYDE                                   | Without HYDE                                |
|------------------------|---------------------------------------------|---------------------------------------------|
| **Documents Found**    | document nr: 0 <br> source: data/week 6.pdf <br> provenance: 0.992 <br> structure of the documents and extract topics (e.g. BERTopic).<br>1) Blei, D. M., Ng, A. Y., & Jordan, M. I. (2003). Latent dirichlet allocation. Journal of machine Learning research, 3(Jan), 993-1022.<br>2) Grootendorst, M. (2022). BERTopic: Neural topic modeling with a class-based TF-IDF procedure. arXiv preprint arXiv:2203.05794.<br>3) Landauer, T. K., Foltz, P. W., & Laham, D. (1998). An introduction to latent semantic analysis. Discourse processes, 25(2-3), 259-284. <br> <br>document nr: 1 <br> source: data/Week 6 reading b -2021-SSCI-AComparativeStudyOfFuzzyTopicModelsAndLDAInTermsOfInterpretability.pdf <br> provenance: 0.988 <br> [3] Blei, D. M., Ng, A. Y ., & Jordan, M. I. (2003). Latent dirichlet<br>allocation. the Journal of machine Learning research, 3, 993-1022.<br>[4] Blei, D. M., & Lafferty, J. D. (2006). Dynamic topic models. In<br>Proceedings of the 23rd international conference on Machine learning<br>(pp. 113-120).<br>[5] Blei, D., & Lafferty, J. (2006). Correlated topic models. Advances in<br>neural information processing systems, 18, 147.<br>[6] Blei, D. M. (2012). Probabilistic topic models. Communications of the<br>ACM, 55(4), 77-84. <br> <br>document nr: 2 <br> source: data/Week 6 reading b -2021-SSCI-AComparativeStudyOfFuzzyTopicModelsAndLDAInTermsOfInterpretability.pdf <br> provenance: 0.958 <br> based topic model [37]. For documents with more words, the<br>Topics over Time model [48] and Dynamic Topic Model [4]<br>are suitable for capturing changes of topics over time. The<br>Pachinko Allocation Model [22] and the Correlated Topic<br>Model [5] are suitable when there is an interest in the<br>correlation between topics.<br>The most popular topic modelling technique is called Latent<br>Dirichlet Allocation (LDA) [3]. LDA performs well on data<br>documents with more than 50 words and no complex topic <br> <br>document nr: 3 <br> source: data/Week 6 reading b -2021-SSCI-ACom                            | document nr: 0 <br> source: data/week 6.pdf <br> provenance: 0.937 <br> JM2050 Natural Language Processing 2024 –2025<br>Uzay Kaymak 6<br>What are topic models<br>Topic model = an (unsupervised) statistical model to discover latent <br>topics from a corpus.<br>6<br>Input:<br>corpus + number of topics<br>Output:<br>Two matrices:<br>1. P(word\|topic) (words x topics)<br>2. P(topic\|document)    (topics x documents)<br>Per topic, the top nwords are typically used to <br>represent a topic<br>Traditional topic modeling <br>algorithms return two matrices. This need not be the case for all models. <br> <br>document nr: 1 <br> source: data/Week 8 Tutorial Topic Modeling.pdf <br> provenance: 0.934 <br> BERTopic<br>For the first ten documents in topic_assigned_to_doc, <br>compare the produced topic to the document. Do you agree this <br>document reflects the topic well?<br>What do you think -1 means?<br>How many words per topic do you have? What is the effect on the <br>topic quality if you work with more topics? <br> <br>document nr: 2 <br> source: data/Week 6 reading b -2021-SSCI-AComparativeStudyOfFuzzyTopicModelsAndLDAInTermsOfInterpretability.pdf <br> provenance: 0.879 <br> The ﬁrst step in developing a topic-based text classiﬁcation<br>algorithm is selecting a suitable topic model. A good topic<br>model should both be interpretable and leading to high classiﬁ-<br>cation performance. Typically, classiﬁcation models use many<br>(at least 50) topics [19], [40]. Therefore, topic models must<br>remain coherent as the number of topics increase.<br>Many different topic models exist, all having a speciﬁc<br>purpose [47]. Applicable topic models for documents with less <br> <br>document nr: 3 <br> source: data/Week 6 reading c 2023-IFSA-InterpretingTopicModelsWithChatGPT.pdf <br> provenance: 0 <br> deterministic algorithms. Although there is growing literature<br>about LLM explainbility [40]–[42], there is no guarantee that<br>the produced output is truthful. For this reason, th                         |
| **LLM's Final Answer** | Based on the provided documents, I found that the Latent Dirichlet Allocation (LDA) [3] is mentioned as performing well on data documents with more than 50 words and no complex topic structures (Document 2). However, it does not provide information about the speed of the system.<br><br>To find the answer, I also looked at Document 1, which mentions several topic modeling techniques, including Latent Dirichlet Allocation (LDA) [3], Dynamic Topic Model [4], and Correlated Topic Model [5]. Unfortunately, none of these documents explicitly state the speed of these systems.<br><br>However, I did find some related information in Document 2, where it states that "For documents with more words, the Topics over Time model [48] and Dynamic Topic Model [4] are suitable for capturing changes of topics over time." This implies that the Dynamic Topic Model might be faster or more efficient for large datasets due to its ability to capture temporal changes.<br><br>Unfortunately, without explicit information about the speed of these systems, I cannot definitively say which one is the fastest. If you need further clarification or details about the speed of these topic modeling systems, please let me know, and I'll try to help you find the necessary information.                          | According to Document 4, which compares the computation time (wall time) of various topic models on the Trump dataset, we can see that some topic models like CTM (Computationally Efficient Topic Model) are faster than others.<br><br>However, based on the information provided, I cannot definitively say which topic modeling system is the absolute fastest. But, according to the graph shown in Document 4, CTM seems to perform better in terms of speed when dealing with larger vocabularies.<br><br>Source: Document 4, id: 358113e00fc16c479a0cdc8511784590, pk: 454311103152261756, source: data/Week 6 reading a - BERTopic-NeuralTopicModelingWithAClassBasedTF-IDFprocedure (1).pdf                       |
| **Time Taken**         | 40.54 seconds                            | 16.05 seconds                         |

### Question 8: Word representations across different languages using SkipGram

| Field                  | With HYDE                                   | Without HYDE                                |
|------------------------|---------------------------------------------|---------------------------------------------|
| **Documents Found**    | document nr: 0 <br> source: data/Week 6 reading a - BERTopic-NeuralTopicModelingWithAClassBasedTF-IDFprocedure (1).pdf <br> provenance: 0.997 <br> 2020). CTM, for example, demonstrates the ad-<br>vantage of relying on pre-trained language models,<br>namely that future improvements in language mod-<br>els may translate into better topic models (Bianchi<br>et al., 2020a).<br>Several approaches have started simplifying the<br>topic building process by clustering word- and<br>document embeddings (Sia et al., 2020; Angelov,<br>2020). This clustering approach allows for a ﬂex-<br>ible topic model as the generation of the clusters<br>can be separated from the process of generating the <br> <br>document nr: 1 <br> source: data/Week 6 reading a - BERTopic-NeuralTopicModelingWithAClassBasedTF-IDFprocedure (1).pdf <br> provenance: 0.991 <br> convert sentences and paragraphs to dense vector<br>representations using pre-trained language models.<br>It achieves state-of-the-art performance on various<br>sentence embedding tasks (Reimers and Gurevych,<br>2020; Thakur et al., 2020).<br>These embeddings, however, are primarily used<br>to cluster semantically similar documents and not<br>directly used in generating the topics. Any other<br>embedding technique can be used for this purpose<br>if the language model generating the document em- <br> <br>document nr: 2 <br> source: data/Week 8 reading.pdf <br> provenance: 0.959 <br> sample the size cbetween 1 and 5. In order to sub-<br>sample the most frequent words, we use a rejection<br>threshold of 10−4 (for more details, see (Mikolov et<br>al., 2013b)). When building the word dictionary, we<br>keep the words that appear at least 5 times in the<br>training set. The step size γ0 is set to 0.025 for the<br>skipgram baseline and to 0.05 for both our model<br>and the cbow baseline. These are the default values<br>in the word2vec package and work well for our<br>model too. <br> <br>document nr: 3 <br> source: data/Week 3 reading c - Mykolo et                            | document nr: 0 <br> source: data/Week 8 reading.pdf <br> provenance: 0.973 <br> Enriching Word Vectors with Subword Information<br>Piotr Bojanowski∗and Edouard Grave∗and Armand Joulin and Tomas Mikolov<br>Facebook AI Research<br>{bojanowski,egrave,ajoulin,tmikolov}@fb.com<br>Abstract<br>Continuous word representations, trained on<br>large unlabeled corpora are useful for many<br>natural language processing tasks. Popular<br>models that learn such representations ignore<br>the morphology of words, by assigning a dis-<br>tinct vector to each word. This is a limitation,<br>especially for languages with large vocabular- <br> <br>document nr: 1 <br> source: data/Week 8 reading.pdf <br> provenance: 0.947 <br> forms, while the Finnish language has ﬁfteen cases<br>for nouns. These languages contain many word<br>forms that occur rarely (or not at all) in the training<br>corpus, making it difﬁcult to learn good word rep-<br>resentations. Because many word formations follow<br>rules, it is possible to improve vector representations<br>for morphologically rich languages by using charac-<br>ter level information.<br>In this paper, we propose to learn representations<br>for character n-grams, and to represent words as the <br> <br>document nr: 2 <br> source: data/Week 8 reading.pdf <br> provenance: 0.929 <br> subword information. Our approach, which incor-<br>porates character n-grams into the skipgram model,<br>is related to an idea that was introduced by Schütze<br>(1993). Because of its simplicity, our model trains<br>fast and does not require any preprocessing or super-<br>vision. We show that our model outperforms base-<br>lines that do not take into account subword informa-<br>tion, as well as methods relying on morphological<br>analysis. We will open source the implementation <br> <br>document nr: 3 <br> source: data/Week 4 tutorial.pdf <br> provenance: 0 <br> Word2Vec: Continuous Bag of Words<br>• Task: Given a context, predict the word<br>This is a visual comparison<br>h                         |
| **LLM's Final Answer** | According to Document 2, the Skip-Gram model is mentioned as one of the baselines used in the study, where the step size γ0 is set to 0.025 for the skipgram baseline.<br><br>Source: Document 2, id: 99162db67c15bc1fdc65739f7877514b                          | To address the question regarding word representations across different languages using SkipGram, I'll rely on Document 1 and Document 10.<br><br>According to Document 1, titled "Enriching Word Vectors with Subword Information," the authors propose a method to improve vector representations for morphologically rich languages by incorporating character-level information. They suggest representing words as the sum of their constituent character n-grams, which helps capture the morphology of words.<br><br>Similarly, Document 10 discusses a new approach based on the SkipGram model, where each word is represented as a bag of character n-grams. This method enables computing word representations for words that didn't appear in the training data and allows for faster training on large corpora.<br><br>Both documents highlight the importance of considering subword information when learning word representations, especially for languages with complex morphology. By doing so, the models can better capture the relationships between words and their components, leading to improved performance in various NLP tasks.<br><br>Source: Documents 1 and 10, specifically "Enriching Word Vectors with Subword Information" and "A New Approach Based on the Skipgram Model."                       |
| **Time Taken**         | 36.7 seconds                            | 17.87 seconds                         |

### Question 9: How did the researchers use attention in transformers?

| Field                  | With HYDE                                   | Without HYDE                                |
|------------------------|---------------------------------------------|---------------------------------------------|
| **Documents Found**    | document nr: 0 <br> source: data/Week 9 reading.pdf <br> provenance: 0.965 <br> position in the decoder to attend over all positions in the input sequence. This mimics the<br>typical encoder-decoder attention mechanisms in sequence-to-sequence models such as<br>[38, 2, 9].<br>• The encoder contains self-attention layers. In a self-attention layer all of the keys, values<br>and queries come from the same place, in this case, the output of the previous layer in the<br>encoder. Each position in the encoder can attend to all positions in the previous layer of the<br>encoder. <br> <br>document nr: 1 <br> source: data/Week 9 reading.pdf <br> provenance: 0.859 <br> query with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the<br>values.<br>In practice, we compute the attention function on a set of queries simultaneously, packed together<br>into a matrix Q. The keys and values are also packed together into matrices Kand V. We compute<br>the matrix of outputs as:<br>Attention(Q,K,V ) = softmax(QKT<br>√dk<br>)V (1)<br>The two most commonly used attention functions are additive attention [2], and dot-product (multi- <br> <br>document nr: 2 <br> source: data/Week 9 reading.pdf <br> provenance: 0.8 <br> textual entailment and learning task-independent sentence representations [4, 27, 28, 22].<br>End-to-end memory networks are based on a recurrent attention mechanism instead of sequence-<br>aligned recurrence and have been shown to perform well on simple-language question answering and<br>language modeling tasks [34].<br>To the best of our knowledge, however, the Transformer is the ﬁrst transduction model relying<br>entirely on self-attention to compute representations of its input and output without using sequence- <br> <br>document nr: 3 <br> source: data/Week 9 reading.pdf <br> provenance: 0 <br> 3.2 Attention<br>An attention function can be described as mapping a query and a set of key-value pairs to an output,<br>where the query, keys, va                            | document nr: 0 <br> source: data/Week 8 Tutorial Topic Modeling.pdf <br> provenance: 0.964 <br> For FLSA-W and LDA<br>Train topic models with both algorithms.<br>• Use an iterative process in which you:<br>• train a topic model,<br>• remove useless words from the corpus ,<br>• retrain your model.<br>• How many iterations did it take to find your topics?<br>• What is the effect on the coherence score?<br>A few considerations:<br>• Setting the number of words per topics: Which number seems most intuitive?<br>• Setting the number of topics: How did you select this number? <br> <br>document nr: 1 <br> source: data/Week 12 b deep_learning_3.pdf <br> provenance: 0.919 <br> Attention modelsSo what can we use these Transformers for? <br> <br>document nr: 2 <br> source: data/Week 7.pdf <br> provenance: 0.781 <br> JM2050 Natural Language Processing 2024 – 2025<br>U. Kaymak 5<br>What is Reasoning?<br>Reasoning: the process of thinking about things in a logical way; opinions <br>and ideas that are based on logical thinking (Oxford Learner’s Dictionary)<br>5<br>Does the ant example demonstrate reasoning skills? How do we know the <br>model did not just learned this problem from the web? How can one test <br>reasoning skills? <br> <br>document nr: 3 <br> source: data/Week 10deep_learning_2.pdf <br> provenance: 0 <br> Attention modelsThis is the transformers architectureLet’s break this down<br>So then finallyWhat do these bits do?! <br> <br>document nr: 4 <br> source: data/Week 7.pdf <br> provenance: 0 <br> JM2050 Natural Language Processing 2024 – 2025<br>U. Kaymak 9<br>How did we get here?<br>9 <br> <br>document nr: 5 <br> source: data/Week 10deep_learning_2.pdf <br> provenance: 0 <br> Attention modelsThis paper led to a family of deep learning networks called “Transformers” <br> <br>document nr: 6 <br> source: data/Week 4 reading b - 24_Dialogue_May_6_2021.pdf <br> provenance: 0 <br> Participant evaluationHuman chats with model for 6 turns and rates 8 dimensions of quality:•av                         |
| **LLM's Final Answer** | According to Document 1, researchers used the attention function in transformers by computing it on a set of queries simultaneously, packed together into a matrix Q, and applying a softmax function to obtain the weights on the values. They divided each query by √dk and multiplied it with the keys matrix K, then applied another softmax function to get the weights on the keys. Finally, they took the dot product of these weights with the values matrix V.<br><br>Source: Document 1, "data/Week 9 reading.pdf", id: f96c10482e0aa364ea5b285e155621d2                          | According to Document 3 and Document 7, researchers used attention in transformers by adding "positional" encoding to the inputs (Document 12). This means they incorporated a mechanism to account for the order of words or tokens in the input sequence, allowing the model to better understand the context and relationships between different parts of the text.<br><br>Additionally, as mentioned in Document 9, attention is a key component of transformers, enabling the model to focus on specific parts of the input when generating output. The attention mechanism allows the model to weigh the importance of different input elements and selectively attend to the most relevant ones.<br><br>Source: Documents 3, 7, and 9.                       |
| **Time Taken**         | 37.63 seconds                            | 13.44 seconds                         |

