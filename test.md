### Question 8: Word representations across different languages using SkipGram

| Field                  | With HYDE                                   | Without HYDE                                |
|------------------------|---------------------------------------------|---------------------------------------------|
| **Documents Found**    | document nr: 0 <br> source: data/Week 6 reading a - BERTopic-NeuralTopicModelingWithAClassBasedTF-IDFprocedure (1).pdf <br> provenance: 0.997 <br> 2020). CTM, for example, demonstrates the ad-<br>vantage of relying on pre-trained language models,<br>namely that future improvements in language mod-<br>els may translate into better topic models (Bianchi<br>et al., 2020a).<br>Several approaches have started simplifying the<br>topic building process by clustering word- and<br>document embeddings (Sia et al., 2020; Angelov,<br>2020). This clustering approach allows for a ﬂex-<br>ible topic model as the generation of the clusters<br>can be separated from the process of generating the <br> <br>document nr: 1 <br> source: data/Week 6 reading a - BERTopic-NeuralTopicModelingWithAClassBasedTF-IDFprocedure (1).pdf <br> provenance: 0.991 <br> convert sentences and paragraphs to dense vector<br>representations using pre-trained language models.<br>It achieves state-of-the-art performance on various<br>sentence embedding tasks (Reimers and Gurevych,<br>2020; Thakur et al., 2020).<br>These embeddings, however, are primarily used<br>to cluster semantically similar documents and not<br>directly used in generating the topics. Any other<br>embedding technique can be used for this purpose<br>if the language model generating the document em- <br> <br>document nr: 2 <br> source: data/Week 8 reading.pdf <br> provenance: 0.959 <br> sample the size cbetween 1 and 5. In order to sub-<br>sample the most frequent words, we use a rejection<br>threshold of 10−4 (for more details, see (Mikolov et<br>al., 2013b)). When building the word dictionary, we<br>keep the words that appear at least 5 times in the<br>training set. The step size γ0 is set to 0.025 for the<br>skipgram baseline and to 0.05 for both our model<br>and the cbow baseline. These are the default values<br>in the word2vec package and work well for our<br>model too. <br> <br>document nr: 3 <br> source: data/Week 3 reading c - Mykolo etal (2013)-Word2Vec.pdf <br> provenance: 0 <br> 6 Conclusion<br>In this paper we studied the quality of vector representations of words derived by various models on<br>a collection of syntactic and semantic language tasks. We observed that it is possible to train high<br>quality word vectors using very simple model architectures, compared to the popular neural network<br>models (both feedforward and recurrent). Because of the much lower computational complexity, it<br>is possible to compute very accurate high dimensional word vectors from a much larger data set. <br> <br>                            | document nr: 0 <br> source: data/Week 8 reading.pdf <br> provenance: 0.973 <br> Enriching Word Vectors with Subword Information<br>Piotr Bojanowski∗and Edouard Grave∗and Armand Joulin and Tomas Mikolov<br>Facebook AI Research<br>{bojanowski,egrave,ajoulin,tmikolov}@fb.com<br>Abstract<br>Continuous word representations, trained on<br>large unlabeled corpora are useful for many<br>natural language processing tasks. Popular<br>models that learn such representations ignore<br>the morphology of words, by assigning a dis-<br>tinct vector to each word. This is a limitation,<br>especially for languages with large vocabular- <br> <br>document nr: 1 <br> source: data/Week 8 reading.pdf <br> provenance: 0.947 <br> forms, while the Finnish language has ﬁfteen cases<br>for nouns. These languages contain many word<br>forms that occur rarely (or not at all) in the training<br>corpus, making it difﬁcult to learn good word rep-<br>resentations. Because many word formations follow<br>rules, it is possible to improve vector representations<br>for morphologically rich languages by using charac-<br>ter level information.<br>In this paper, we propose to learn representations<br>for character n-grams, and to represent words as the <br> <br>document nr: 2 <br> source: data/Week 8 reading.pdf <br> provenance: 0.929 <br> subword information. Our approach, which incor-<br>porates character n-grams into the skipgram model,<br>is related to an idea that was introduced by Schütze<br>(1993). Because of its simplicity, our model trains<br>fast and does not require any preprocessing or super-<br>vision. We show that our model outperforms base-<br>lines that do not take into account subword informa-<br>tion, as well as methods relying on morphological<br>analysis. We will open source the implementation <br> <br>document nr: 3 <br> source: data/Week 4 tutorial.pdf <br> provenance: 0 <br> Word2Vec: Continuous Bag of Words<br>• Task: Given a context, predict the word<br>This is a visual comparison<br>https://kavita-ganesan.com/comparison-between-<br>cbow-skipgram-subword/ <br> <br>document nr: 4 <br> source: data/Week 6 reading a - BERTopic-NeuralTopicModelingWithAClassBasedTF-IDFprocedure (1).pdf <br> provenance: 0 <br> time and the extent to which topic representations<br>reﬂect that.<br>In BERTopic, we can model this behavior by<br>leveraging the c-TF-IDF representations of topics.<br>Here, we assume that the temporal nature of topics<br>should not inﬂuence the creation of global topics.<br>The same topic might appear across different times,<br>albeit possibly represented differently. As an exam-<br>ple, a global topic about cars might contain words<br>such as "car" and "vehicle" regardless of the tem- <br> <br>document nr: 5 <br> source: data/Week 3.pdf <br> provenance: 0 <br> JM2050 Natural Language Processing 2024 – 2025<br>Uzay Kaymak 53<br>www.jads.nl<br>The Bag of Words Representation <br> <br>document nr: 6 <br> source: data/Week 8 reading.pdf <br> provenance: 0 <br> similar representations. Soricut and Och (2015)<br>described a method to learn vector representations<br>of morphological transformations, allowing to ob-<br>tain representations for unseen words by applying<br>these rules. Word representations trained on mor-<br>phologically annotated data were introduced by Cot-<br>terell and Schütze (2015). Closest to our approach,<br>Schütze (1993) learned representations of character<br>four-grams through singular value decomposition,<br>and derived representations for words by summing <br> <br>document nr: 7 <br> source: data/Week 3 reading b - vectorsemantics2024.pdf <br> provenance: 0 <br> embeddings for each target word and context word in the vocabulary. Let’s now turn<br>to learning these embeddings (which is the real goal of training this classiﬁer in the<br>ﬁrst place).<br>6.8.2 Learning skip-gram embeddings<br>Word2vec learns embeddings by starting with an initial set of embedding vectors<br>and then iteratively shifting the embedding of each word w to be more like the em-<br>beddings of words that occur nearby in texts, and less like the embeddings of words <br> <br>document nr: 8 <br> source: data/Week 3 reading c - Mykolo etal (2013)-Word2Vec.pdf <br> provenance: 0 <br> 1.2 Previous Work<br>Representation of words as continuous vectors has a long history [10, 26, 8]. A very popular model<br>architecture for estimating neural network language model (NNLM) was proposed in [1], where a<br>feedforward neural network with a linear projection layer and a non-linear hidden layer was used to<br>learn jointly the word vector representation and a statistical language model. This work has been<br>followed by many others. <br> <br>document nr: 9 <br> source: data/Week 3 reading b - vectorsemantics2024.pdf <br> provenance: 0 <br> jam<br>apricot<br>c pos<br>matrix<br>Tolstoy move apricot  and Tolstoy  apart<br>decreasing c neg2  z w<br>!<br>c neg1<br>c neg2<br>k=2<br>Figure 6.14 Intuition of one step of gradient descent. The skip-gram model tries to shift<br>embeddings so the target embeddings (here for apricot ) are closer to (have a higher dot prod-<br>uct with) context embeddings for nearby words (here jam ) and further from (lower dot product<br>with) context embeddings for noise words that don’t occur nearby (here Tolstoy and matrix ). <br> <br>document nr: 10 <br> source: data/Week 8 reading.pdf <br> provenance: 0 <br> ies and many rare words. In this paper, we pro-<br>pose a new approach based on the skipgram<br>model, where each word is represented as a<br>bag of character n-grams. A vector represen-<br>tation is associated to each character n-gram;<br>words being represented as the sum of these<br>representations. Our method is fast, allow-<br>ing to train models on large corpora quickly<br>and allows us to compute word representations<br>for words that did not appear in the training<br>data. We evaluate our word representations on <br> <br>document nr: 11 <br> source: data/Week 8 reading.pdf <br> provenance: 0 <br> model too.<br>Using this setting on English data, our model with<br>character n-grams is approximately 1.5× slower<br>to train than the skipgram baseline. Indeed,<br>we process 105k words/second/thread versus 145k<br>words/second/thread for the baseline. Our model is<br>implemented in C++, and is publicly available.3<br>4.4 Datasets<br>Except for the comparison to previous<br>work (Sec. 5.3), we train our models on Wikipedia<br>data.4 We downloaded Wikipedia dumps in nine<br>languages: Arabic, Czech, German, English, <br> <br>document nr: 12 <br> source: data/Week 3 reading b - vect                         |
| **LLM's Final Answer** | According to Document 2, the Skip-Gram model is mentioned as one of the baselines used in the study, where the step size γ0 is set to 0.025 for the skipgram baseline.<br><br>Source: Document 2, id: 99162db67c15bc1fdc65739f7877514b                          | To address the question regarding word representations across different languages using SkipGram, I'll rely on Document 1 and Document 10.<br><br>According to Document 1, titled "Enriching Word Vectors with Subword Information," the authors propose a method to improve vector representations for morphologically rich languages by incorporating character-level information. They suggest representing words as the sum of their constituent character n-grams, which helps capture the morphology of words.<br><br>Similarly, Document 10 discusses a new approach based on the SkipGram model, where each word is represented as a bag of character n-grams. This method enables computing word representations for words that didn't appear in the training data and allows for faster training on large corpora.<br><br>Both documents highlight the importance of considering subword information when learning word representations, especially for languages with complex morphology. By doing so, the models can better capture the relationships between words and their components, leading to improved performance in various NLP tasks.<br><br>Source: Documents 1 and 10, specifically "Enriching Word Vectors with Subword Information" and "A New Approach Based on the Skipgram Model."                       |
| **Time Taken**         | 36.7 seconds                            | 17.87 seconds                         |

